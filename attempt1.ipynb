{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"kaggle_eegs\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.enabled\", \"true\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/harraz/my_tensorflow/venv/eegs/training_data/train.csv'\n",
    "\n",
    "# Read labels CSV file into a DataFrame\n",
    "train_y = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "train_y.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "file_path = '/home/harraz/my_tensorflow/venv/eegs/training_data/*.parquet'\n",
    "\n",
    "# Read parquet file into a DataFrame\n",
    "train_X = spark.read.parquet(file_path, inferSchema=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "train_Labels = train_y.withColumn('expert_consensus', when(train_y['expert_consensus'] == 'Seizure', 1).otherwise(0))\n",
    "train_Labels.select(train_Labels.expert_consensus).show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# Assuming you have a SparkSession named 'spark'\n",
    "\n",
    "# List of feature columns (excluding the 'EKG' column)\n",
    "feature_columns = ['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2']\n",
    "\n",
    "# Convert columns to DoubleType\n",
    "for col_name in feature_columns:\n",
    "    train_Xx = train_X.withColumn(col_name, col(col_name).cast(DoubleType()))\n",
    "\n",
    "\n",
    "# Create a VectorAssembler\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_assembled\")\n",
    "\n",
    "# Create a StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"normalized_features\", withStd=True, withMean=True)\n",
    "\n",
    "# Create a Pipeline\n",
    "pipeline = Pipeline(stages=[assembler, scaler])\n",
    "\n",
    "# Fit the pipeline on the training data\n",
    "pipeline_model = pipeline.fit(train_Xx)\n",
    "\n",
    "# Transform the training data\n",
    "train_X_scaled = pipeline_model.transform(train_Xx)\n",
    "\n",
    "# Display the scaled features\n",
    "train_X_scaled.select(\"normalized_features\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, DoubleType\n",
    "\n",
    "# Assuming \"normalized_features\" is a VectorUDT\n",
    "vector_to_array_udf = udf(lambda v: v.toArray().tolist(), ArrayType(DoubleType()))\n",
    "\n",
    "# Apply the UDF to convert \"normalized_features\" to an array\n",
    "train_X_scaled = train_X_scaled.withColumn(\"normalized_features_array\", vector_to_array_udf(\"normalized_features\"))\n",
    "\n",
    "# Now, you can use the array as needed, and potentially convert it to a NumPy array\n",
    "train_X_scaled.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "\n",
    "# Assuming train_X_scaled_np and train_Labels are PySpark DataFrames\n",
    "# Convert them directly to NumPy arrays\n",
    "train_X_scaled_np = np.array(train_X_scaled.collect())\n",
    "train_Labels_np = np.array(train_Labels.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)\n",
    "# build keras model with optimizer and 3 layer NN\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    layers.Dense(300, activation=tf.nn.relu, input_shape=[18]),\n",
    "    layers.Dense(200, activation=tf.nn.relu),\n",
    "    layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "  ])\n",
    "\n",
    "  # optimizer = tf.keras.optimizers.RMSprop(0.05)\n",
    "\n",
    "  # model.compile(loss='mse',\n",
    "  #               optimizer=optimizer,\n",
    "  #               metrics=['mse','mae'])\n",
    "  \n",
    "  model.compile(loss=keras.losses.mean_absolute_error,\n",
    "              optimizer=keras.optimizers.SGD(0.1),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "model=build_model()\n",
    "model.summary()\n",
    "# keras.utils.plot_model(model, \"hd_model.png\", show_shapes=True)\n",
    "\n",
    "EPOCHS = 300\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=100)\n",
    "history = model.fit(\n",
    "    x=train_X_scaled_np, y=train_Labels, batch_size=None, epochs=EPOCHS, verbose=1,\n",
    "    callbacks=[early_stop], validation_split=0.2, validation_data=None, shuffle=True,\n",
    "    class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None,\n",
    "    validation_steps=None, validation_batch_size=None, validation_freq=1,\n",
    "    max_queue_size=10, workers=1, use_multiprocessing=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "# # Define the schema struct with DoubleType\n",
    "# schema_struct_double = StructType([\n",
    "#     StructField(\"Fp1\", DoubleType(), True),\n",
    "#     StructField(\"F3\", DoubleType(), True),\n",
    "#     StructField(\"C3\", DoubleType(), True),\n",
    "#     StructField(\"P3\", DoubleType(), True),\n",
    "#     StructField(\"F7\", DoubleType(), True),\n",
    "#     StructField(\"T3\", DoubleType(), True),\n",
    "#     StructField(\"T5\", DoubleType(), True),\n",
    "#     StructField(\"O1\", DoubleType(), True),\n",
    "#     StructField(\"Fz\", DoubleType(), True),\n",
    "#     StructField(\"Cz\", DoubleType(), True),\n",
    "#     StructField(\"Pz\", DoubleType(), True),\n",
    "#     StructField(\"Fp2\", DoubleType(), True),\n",
    "#     StructField(\"F4\", DoubleType(), True),\n",
    "#     StructField(\"C4\", DoubleType(), True),\n",
    "#     StructField(\"P4\", DoubleType(), True),\n",
    "#     StructField(\"F8\", DoubleType(), True),\n",
    "#     StructField(\"T4\", DoubleType(), True),\n",
    "#     StructField(\"T6\", DoubleType(), True),\n",
    "#     StructField(\"O2\", DoubleType(), True),\n",
    "#     StructField(\"EKG\", DoubleType(), True)\n",
    "# ])\n",
    "# # Use the VectorAssembler with explicit input and output columns\n",
    "# assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "# train_X = assembler.transform(train_X)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
