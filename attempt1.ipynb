{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# import pyarrow\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.appName(\"kaggle_eegs\").getOrCreate()\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"True\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/home/harraz/my_tensorflow/venv/eegs/training_data/train.csv'\n",
    "\n",
    "# Read labels CSV file into a DataFrame\n",
    "train_y = spark.read.csv(file_path, header=True, inferSchema=True)\n",
    "\n",
    "# Show the DataFrame\n",
    "train_y.show(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "\n",
    "file_path = '/home/harraz/my_tensorflow/venv/eegs/training_data/*.parquet'\n",
    "\n",
    "# Read parquet file into a DataFrame\n",
    "train_X = spark.read.parquet(file_path, inferSchema=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import when\n",
    "\n",
    "train_Labels = train_y.withColumn('expert_consensus', when(train_y['expert_consensus'] == 'Seizure', 1).otherwise(0))\n",
    "train_Labels = train_Labels.select(train_Labels.expert_consensus).limit(500000)\n",
    "train_Labels.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler, StandardScaler\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "from pyspark.sql.functions import col\n",
    "# from pyspark.sql.types import DoubleType\n",
    "\n",
    "# List of feature columns\n",
    "feature_columns = ['Fp1', 'F3', 'C3', 'P3', 'F7', 'T3', 'T5', 'O1', 'Fz', 'Cz', 'Pz', 'Fp2', 'F4', 'C4', 'P4', 'F8', 'T4', 'T6', 'O2', 'EKG']\n",
    "\n",
    "# Create a VectorAssembler\n",
    "# assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_assembled\")\n",
    "assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features_assembled\", handleInvalid=\"skip\")\n",
    "\n",
    "# Apply VectorAssembler to the training data\n",
    "train_X_assembled = assembler.transform(train_X)\n",
    "\n",
    "# Cast the features_assembled column to VectorUDT\n",
    "train_X_assembled = train_X_assembled.withColumn(\"features_assembled\", col(\"features_assembled\").cast(VectorUDT()))\n",
    "\n",
    "# Create a StandardScaler\n",
    "scaler = StandardScaler(inputCol=\"features_assembled\", outputCol=\"normalized_features\", withStd=True, withMean=True)\n",
    "\n",
    "# Fit the StandardScaler on the training data\n",
    "scaler_model = scaler.fit(train_X_assembled)\n",
    "\n",
    "# Transform the training data\n",
    "train_X_scaled = scaler_model.transform(train_X_assembled)\n",
    "\n",
    "# Display the scaled features\n",
    "train_X_scaled.select(\"normalized_features\").show(truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the DataFrame to a Parquet file\n",
    "# train_X_scaled.write.parquet(\"/home/harraz/my_tensorflow/venv/eegs/training_data/train_X_scaled.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X_scaled = spark.read.parquet(\"/home/harraz/my_tensorflow/venv/eegs/training_data/train_X_scaled.parquet\")\n",
    "train_X_scaled = train_X_scaled.select('normalized_features').limit(500000)\n",
    "train_X_scaled.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import ArrayType, FloatType, DoubleType\n",
    "from pyspark.ml.linalg import VectorUDT\n",
    "\n",
    "# Assuming \"normalized_features\" is a VectorUDT\n",
    "vector_to_array_udf = udf(lambda v: v.toArray().tolist(), ArrayType(FloatType()))\n",
    "\n",
    "# Apply the UDF to convert \"normalized_features\" to an array\n",
    "train_X_scaled = train_X_scaled.withColumn(\"normalized_features_array\", vector_to_array_udf(\"normalized_features\"))\n",
    "\n",
    "train_X_scaled =train_X_scaled.drop('normalized_features')\n",
    "# Now, you can use the array as needed, and potentially convert it to a NumPy array\n",
    "train_X_scaled.show(3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming train_X_scaled_np and train_Labels are PySpark DataFrames\n",
    "# Convert them directly to NumPy arrays\n",
    "train_X_scaled_np = np.array(train_X_scaled.collect())\n",
    "train_Labels_np = np.array(train_Labels.collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(train_X_scaled_np.shape)\n",
    "print(train_Labels_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "print(tf.__version__)\n",
    "# build keras model with optimizer and 3 layer NN\n",
    "def build_model():\n",
    "  model = keras.Sequential([\n",
    "    tf.keras.layers.Flatten(input_shape=(1, 20)),  # Adjust the input shape\n",
    "    # tf.keras.layers.GlobalAveragePooling1D(input_shape=(1, 20)),  # Replace Flatten with GlobalAveragePooling1D\n",
    "    layers.Dense(300, activation=tf.nn.relu),\n",
    "    layers.Dense(200, activation=tf.nn.relu),\n",
    "    layers.Dense(100, activation=tf.nn.relu),\n",
    "    layers.Dense(1, activation=tf.nn.sigmoid)\n",
    "  ])\n",
    "\n",
    "  # optimizer = tf.keras.optimizers.RMSprop(0.05)\n",
    "\n",
    "  # model.compile(loss='mse',\n",
    "  #               optimizer=optimizer,\n",
    "  #               metrics=['mse','mae'])\n",
    "  \n",
    "  model.compile(loss='binary_crossentropy', \n",
    "                optimizer='adam', \n",
    "                metrics=['accuracy'], )\n",
    "\n",
    "  # model.compile(loss=keras.losses.mean_absolute_error,\n",
    "  #               optimizer=keras.optimizers.SGD(0.4),\n",
    "  #               metrics=['accuracy'])\n",
    "\n",
    "  return model\n",
    "\n",
    "model=build_model()\n",
    "model.summary()\n",
    "# keras.utils.plot_model(model, \"hd_model.png\", show_shapes=True)\n",
    "\n",
    "EPOCHS = 400\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='accuracy', patience=10)\n",
    "history = model.fit(\n",
    "    x=train_X_scaled_np, y=train_Labels_np, batch_size=100, epochs=EPOCHS, verbose=1,\n",
    "    callbacks=[early_stop], validation_split=0.2, validation_data=None, shuffle=True,\n",
    "    class_weight=None, sample_weight=None, initial_epoch=0, steps_per_epoch=None,\n",
    "    validation_steps=None, validation_batch_size=None, validation_freq=1,\n",
    "    max_queue_size=10, workers=2, use_multiprocessing=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To plot pretty figures\n",
    "import pandas as pd\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.DataFrame(history.history).plot(figsize=(10, 7))\n",
    "plt.grid(True)\n",
    "\n",
    "# Add model parameters as text to the plot\n",
    "model_params_text = (\n",
    "    f\"Loss: {'binary_crossentropy'}\\n\"\n",
    "    f\"Optimizer: {'adam'}\\nEpochs: 400\"\n",
    "        f\"with tf.keras.layers.Flatten\\nBatch Size: 100\\nRun Duration: about 18min\\nWorkers:2\\nRows: 300k\" \n",
    ")\n",
    "\n",
    "plt.text(0.5, 0.4, model_params_text, transform=plt.gca().transAxes,\n",
    "         fontsize=12, verticalalignment='top', bbox=dict(boxstyle='round', alpha=0.2))\n",
    "\n",
    "# plt.gca().set_ylim(0, 1)\n",
    "plt.xticks(np.arange(0, 100, 25))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyspark.sql.types import StructType, StructField, DoubleType\n",
    "\n",
    "# # Define the schema struct with DoubleType\n",
    "# schema_struct_double = StructType([\n",
    "#     StructField(\"Fp1\", DoubleType(), True),\n",
    "#     StructField(\"F3\", DoubleType(), True),\n",
    "#     StructField(\"C3\", DoubleType(), True),\n",
    "#     StructField(\"P3\", DoubleType(), True),\n",
    "#     StructField(\"F7\", DoubleType(), True),\n",
    "#     StructField(\"T3\", DoubleType(), True),\n",
    "#     StructField(\"T5\", DoubleType(), True),\n",
    "#     StructField(\"O1\", DoubleType(), True),\n",
    "#     StructField(\"Fz\", DoubleType(), True),\n",
    "#     StructField(\"Cz\", DoubleType(), True),\n",
    "#     StructField(\"Pz\", DoubleType(), True),\n",
    "#     StructField(\"Fp2\", DoubleType(), True),\n",
    "#     StructField(\"F4\", DoubleType(), True),\n",
    "#     StructField(\"C4\", DoubleType(), True),\n",
    "#     StructField(\"P4\", DoubleType(), True),\n",
    "#     StructField(\"F8\", DoubleType(), True),\n",
    "#     StructField(\"T4\", DoubleType(), True),\n",
    "#     StructField(\"T6\", DoubleType(), True),\n",
    "#     StructField(\"O2\", DoubleType(), True),\n",
    "#     StructField(\"EKG\", DoubleType(), True)\n",
    "# ])\n",
    "# # Use the VectorAssembler with explicit input and output columns\n",
    "# assembler = VectorAssembler(inputCols=feature_columns, outputCol=\"features\")\n",
    "# train_X = assembler.transform(train_X)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Initialize a GlobalAveragePooling1D (GAP1D) layer\n",
    "gap1d_layer = tf.keras.layers.GlobalAveragePooling1D()\n",
    "\n",
    "# Define sample array\n",
    "sample_array = np.array([[[10,2],[1,3],[1,1]]])\n",
    "\n",
    "# Print shape and contents of sample array\n",
    "print(f'shape of sample_array = {sample_array.shape}')\n",
    "print(f'sample array: {sample_array}')\n",
    "\n",
    "# Pass the sample array to the GAP1D layer\n",
    "output = gap1d_layer(sample_array)\n",
    "\n",
    "# Print shape and contents of the GAP1D output array\n",
    "print(f'output shape of gap1d_layer: {output.shape}')\n",
    "print(f'output array of gap1d_layer: {output.numpy()}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
